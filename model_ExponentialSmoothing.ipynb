{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# grid search holt winter's exponential smoothing\n",
    "from math import sqrt\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = pd.read_csv('data/train_fil_3.csv',index_col=0),pd.read_csv('data/validation_fil_3.csv',index_col=0),pd.read_csv('data/test_fil_3.csv',index_col=0)\n",
    "for df in [train_df,validation_df,test_df]:\n",
    "    df.columns = [int(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate MAPE for all observations where y_true is not 0\n",
    "def mape(y_true, y_predict):\n",
    "    '''Returns mean percentage error for all predictions where y_true is not 0. Where y_true is 0, the percentage error is 0 as well '''\n",
    "    return np.mean([np.absolute(y_true[idx] - y_predict[idx])/y_true[idx] * 100 if y_true[idx] != 0 else 0 for idx,_ in enumerate(y_true) ])\n",
    "\n",
    "def median_pe(y_true, y_predict):\n",
    "    '''Returns mean percentage error for all predictions where y_true is not 0. Where y_true is 0, the percentage error is 0 as well '''\n",
    "    return np.median([np.absolute(y_true[idx] - y_predict[idx])/y_true[idx] * 100 if y_true[idx] != 0 else 0 for idx,_ in enumerate(y_true) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-step Holt Winterâ€™s Exponential Smoothing forecast\n",
    "def exp_smoothing_forecast(history, config):\n",
    "\tt,d,s,p,b,r = config\n",
    "\t# define model\n",
    "\thistory = array(history)\n",
    "\tmodel = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)\n",
    "\t# fit model\n",
    "\tmodel_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n",
    "\t# make one step forecast\n",
    "\tyhat = model_fit.predict(len(history), len(history))\n",
    "\treturn yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(train,test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = exp_smoothing_forecast(history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = mape(test, predictions)\n",
    "\treturn error\n",
    " \n",
    "# score a model, return None on failure\n",
    "def score_model(train, test, cfg, debug=False):\n",
    "\tresult = None\n",
    "\t# convert config to a key\n",
    "\tkey = str(cfg)\n",
    "\t# show all warnings and fail on exception if debugging\n",
    "\tif debug:\n",
    "\t\tresult = walk_forward_validation(train, test, cfg)\n",
    "\telse:\n",
    "\t\t# one failure during model validation suggests an unstable config\n",
    "\t\ttry:\n",
    "\t\t\t# never show warnings when grid searching, too noisy\n",
    "\t\t\twith catch_warnings():\n",
    "\t\t\t\tfilterwarnings(\"ignore\")\n",
    "\t\t\t\tresult = walk_forward_validation(train, test, cfg)\n",
    "\t\texcept:\n",
    "\t\t\terror = None\n",
    "\t# check for an interesting result\n",
    "\tif result is not None:\n",
    "\t\tprint(' > Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    " \n",
    "# grid search configs\n",
    "def grid_search(train, test, cfg_list, parallel=True):\n",
    "\tscores = None\n",
    "\tif parallel:\n",
    "\t\t# execute configs in parallel\n",
    "\t\texecutor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n",
    "\t\ttasks = (delayed(score_model)(train, test, cfg) for cfg in cfg_list)\n",
    "\t\tscores = executor(tasks)\n",
    "\telse:\n",
    "\t\tscores = [score_model(train, test, cfg) for cfg in cfg_list]\n",
    "\t# remove empty results\n",
    "\tscores = [r for r in scores if r[1] != None]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    " \n",
    "# create a set of exponential smoothing configs to try\n",
    "def exp_smoothing_configs(seasonal=[None]):\n",
    "\tmodels = list()\n",
    "\t# define config lists\n",
    "\tt_params = ['add', 'mul', None]\n",
    "\td_params = [True, False]\n",
    "\ts_params = ['add', 'mul', None]\n",
    "\tp_params = seasonal\n",
    "\tb_params = [True, False]\n",
    "\tr_params = [True, False]\n",
    "\t# create config instances\n",
    "\tfor t in t_params:\n",
    "\t\tfor d in d_params:\n",
    "\t\t\tfor s in s_params:\n",
    "\t\t\t\tfor p in p_params:\n",
    "\t\t\t\t\tfor b in b_params:\n",
    "\t\t\t\t\t\tfor r in r_params:\n",
    "\t\t\t\t\t\t\tcfg = [t,d,s,p,b,r]\n",
    "\t\t\t\t\t\t\tmodels.append(cfg)\n",
    "\treturn models\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model[['add', True, 'add', 7, False, False]] 9.659\n",
      " > Model[['add', True, 'add', 7, False, True]] 9.665\n",
      " > Model[['add', True, 'add', 365, False, True]] 51.646\n",
      " > Model[['add', True, 'add', 365, False, False]] 51.629\n",
      " > Model[['add', True, None, 7, False, True]] 40.212\n",
      " > Model[['add', True, None, 365, False, True]] 40.212\n",
      " > Model[['add', False, 'add', 7, False, True]] 10.056\n",
      " > Model[['add', False, 'add', 7, False, False]] 10.044\n",
      " > Model[['add', True, None, 7, False, False]] 40.211\n",
      " > Model[['add', True, None, 365, False, False]] 40.211\n",
      " > Model[['add', False, None, 7, False, True]] 41.170\n",
      " > Model[['add', False, None, 7, False, False]] 43.031\n",
      " > Model[['add', False, None, 365, False, True]] 41.170\n",
      " > Model[['add', False, None, 365, False, False]] 43.031\n",
      " > Model[['add', False, 'add', 365, False, True]] 52.897\n",
      " > Model[['add', False, 'add', 365, False, False]] 52.865\n",
      " > Model[[None, False, None, 7, False, False]] 40.144\n",
      " > Model[[None, False, None, 7, False, True]] 40.144\n",
      " > Model[[None, False, 'add', 7, False, True]] 9.595\n",
      " > Model[[None, False, None, 365, False, False]] 40.144\n",
      " > Model[[None, False, None, 365, False, True]] 40.144\n",
      " > Model[[None, False, 'add', 7, False, False]] 9.648\n",
      " > Model[[None, False, 'add', 365, False, True]] 58.283\n",
      " > Model[[None, False, 'add', 365, False, False]] 58.433\n",
      "done\n",
      "[None, False, 'add', 7, False, True] 9.594741561630848\n",
      "[None, False, 'add', 7, False, False] 9.647501354037272\n",
      "['add', True, 'add', 7, False, False] 9.658548315335132\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t# model configs\n",
    "\tcfg_list = exp_smoothing_configs(seasonal=[7,365])\n",
    "\t# grid search\n",
    "\tscores = grid_search(train_df[6], validation_df[6], cfg_list)\n",
    "\tprint('done')\n",
    "\t# list top 3 configs\n",
    "\tfor cfg, error in scores[:3]:\n",
    "\t\tprint(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best Holt Winters parameters:\n",
    "* [None, False, 'add', 7, False, True] 9.594741561630848\n",
    "* [None, False, 'add', 7, False, False] 9.647501354037272\n",
    "* ['add', True, 'add', 7, False, False] 9.658548315335132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_prediction(train,test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = exp_smoothing_forecast(history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = mape(test, predictions)\n",
    "\treturn predictions[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(y_true,y_predict):\n",
    "    '''Returns list with residuals for all observations where y_true not 0. Where y_true is 0, the residuals are 0 as well '''\n",
    "    return [y_true[idx] - y_predict[idx] if y_true[idx] != 0 else 0 for idx,_ in enumerate(y_true) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_residuals(y_true,y_predict):\n",
    "    '''Returns list with percentage errors for all observations where y_true not 0. Where y_true is 0, the percentage error is 0 as well'''\n",
    "    return [(y_true[idx] - y_predict[idx])/y_true[idx] * 100 if y_true[idx] != 0 else 0 for idx,_ in enumerate(y_true) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative density function of residuals\n",
    "def residual_cdf(data):\n",
    "    '''Plots cdf of residual input data'''\n",
    "    # sort the data:\n",
    "    data_sorted = np.sort(data)\n",
    "\n",
    "    # calculate the proportional values of samples\n",
    "    p = 1. * np.arange(len(data)) / (len(data) - 1)\n",
    "\n",
    "    # plot the sorted data:\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "\n",
    "    ax1 = fig.add_subplot(311)\n",
    "    ax1.plot(data_sorted, p)\n",
    "    ax1.set_title('Residuals Cumulative Distribution Function')\n",
    "    ax1.set_xlabel('Residuals');\n",
    "    ax1.set_ylabel('Cumulative Distribution');\n",
    "    ax1.axvline(x=np.percentile(data,5),color='r') \n",
    "    ax1.axvline(x=np.percentile(data,95),color='r')\n",
    "\n",
    "    ax2 = fig.add_subplot(312)\n",
    "    ax2.plot([idx for idx,_ in enumerate(data)],data,'bo');\n",
    "    ax2.plot([idx for idx,_ in enumerate(data)],np.zeros(len(data)),'r-');\n",
    "    ax2.set_title('Residuals over time')\n",
    "    ax2.set_xlabel('Time in days');\n",
    "    ax2.set_ylabel('Residual');  \n",
    "    \n",
    "    #Here, we could also add Q-Q plot and auto correlation plot for the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(y_true,y_predict):\n",
    "    '''Plots true and predicted values on same y-axis'''\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    ax1 = fig.add_subplot(311)\n",
    "    ax1.plot(range(len(y_true)), y_true,'bo')\n",
    "    ax1.plot(range(len(y_predict)),y_predict,'r-')\n",
    "    ax1.set_title('Complete prediction')\n",
    "    \n",
    "    ax2 = fig.add_subplot(312)\n",
    "    ax2.plot(range(len(y_true[:60])), y_true[:60],'bo')\n",
    "    ax2.plot(range(len(y_predict[:60])),y_predict[:60],'r-o')\n",
    "    ax2.set_title('Prediction first 60 days')\n",
    "    ax2.set_ylim(0,max(y_true))\n",
    "    ax3 = fig.add_subplot(313)\n",
    "    ax3.plot(range(len(y_true[-60:])), y_true[-60:],'bo')\n",
    "    ax3.plot(range(len(y_predict[-60:])),y_predict[-60:],'ro-')\n",
    "    ax3.set_title('Prediction last 60 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def management_summary(y_true,y_predict):\n",
    "    data = pd.DataFrame.from_dict({'y_true':y_true, 'y_predict':y_predict})\n",
    "    \n",
    "    #only regard data where y_true is not 0\n",
    "    ex_0 = data[data['y_true'] != 0]\n",
    "    \n",
    "    #calculate how ofter we under- and over-estimate the revenue\n",
    "    pct_lower = round(sum(ex_0.y_predict - ex_0.y_true < 0)/len(ex_0.y_true) * 100,1)\n",
    "    pct_higher = round(100 - pct_lower,1)\n",
    "    \n",
    "    #calculate cumulative sums of under- and over estimation\n",
    "    cumsum_lower = np.cumsum([np.abs(ex_0.y_predict[idx] - ex_0.y_true[idx]) if ex_0.y_predict[idx] < ex_0.y_true[idx] else 0 for idx,y in enumerate(ex_0.y_true) ])\n",
    "    cumsum_higher = np.cumsum([np.abs(ex_0.y_predict[idx] - ex_0.y_true[idx]) if ex_0.y_predict[idx] > ex_0.y_true[idx] else 0 for idx,y in enumerate(ex_0.y_true)])\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.plot(range(len(ex_0.y_true)), cumsum_lower,'b-o')\n",
    "    ax1.plot(range(len(ex_0.y_true)),cumsum_higher,'r-o')\n",
    "    \n",
    "    ax1.set_title('Cumulative Sums of Under- and Over-Estimation')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Cumulated sum of errors')\n",
    "    ax1.legend(['Under Estimation', 'Over Estimation', 'True Values'])\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'black'\n",
    "    ax2.set_ylabel('Measured Values', color = color)\n",
    "    ax2.plot(range(len(ex_0.y_true)),ex_0.y_true,'--', color=color, marker=10)\n",
    "    \n",
    "    return f'The model underestimates {pct_lower}% of the time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b36c41ebcdb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get prediction values for best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwalk_forward_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#get errors of prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#get prediction values for best model\n",
    "prediction = walk_forward_prediction(train, test, scores[0])\n",
    "\n",
    "#get errors of prediction\n",
    "errors = residuals(test,prediction)\n",
    "\n",
    "#get percentage errors of prediction\n",
    "pct_errors = pct_residuals(test,prediction)\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
